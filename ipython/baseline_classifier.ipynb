{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pdb, sys, codecs, pickle\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Add the parent directory to PYTHONPATH so that we can use utils.py\n",
    "sys.path.append('..')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def build_word_vector_matrix(vector_file, n_words=None):\n",
    "#     '''Read a GloVe array from sys.argv[1] and return its vectors and labels as arrays'''\n",
    "#     np_arrays = []\n",
    "#     labels_array = []\n",
    "\n",
    "#     with codecs.open(vector_file, 'r', 'utf-8') as f:\n",
    "#         for i, line in enumerate(f):\n",
    "#             sr = line.split()\n",
    "#             labels_array.append(sr[0])\n",
    "#             np_arrays.append(np.array([float(j) for j in sr[1:]]))\n",
    "#             if n_words and i == n_words - 1:\n",
    "#                 break\n",
    "#         return np.array(np_arrays), labels_array\n",
    "            \n",
    "# parse data and create embedding matrix and labels array\n",
    "embeddings, labels, id_to_word, word_to_id = utils.build_word_vector_matrix('../data/embeddings/2/vectors_d50.txt')\n",
    "\n",
    "# # create dictionaries from labels array for quick lookups\n",
    "# id_to_word = dict(zip(range(len(labels)), labels))\n",
    "# word_to_id = dict((v,k) for k,v in id_to_word.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def move_to_glove(move):\n",
    "    if move in word_to_id:\n",
    "        return embeddings[word_to_id[move]]\n",
    "    else :\n",
    "        # return the \"I've never seen that\" vector\n",
    "#         print('NOT FOUND')\n",
    "        return np.full(len(embeddings[0]), 1, dtype=np.float32) \n",
    "\n",
    "def encode_moves(moves, length):   \n",
    "    encoded = [move_to_glove(m) for m in moves]\n",
    "    while len(encoded) < length:\n",
    "        encoded.append(np.zeros(len(embeddings[0])))\n",
    "    return encoded\n",
    "\n",
    "def load_data(file, num_to_load=None, split_ratio=0.75):\n",
    "    if '.pickle' in file:\n",
    "        # Parsing games with pgnparser takes a while, so I've pickled some\n",
    "        with open(file, 'rb') as f:\n",
    "            games = pickle.load(f)\n",
    "        print('loaded {} games'.format(len(games)))\n",
    "        \n",
    "        # what is the longest game?\n",
    "        longest_game = games[np.argmax([len(g.moves) for g in games])]\n",
    "        max_half_turns = len(longest_game.moves)\n",
    "        print('longest game is {} half turns'.format(max_half_turns))\n",
    "        \n",
    "        X, y = [], []\n",
    "        counter = 0\n",
    "        for game in games:\n",
    "            for i, move in enumerate(game.moves):\n",
    "                if move in word_to_id:\n",
    "                    # for each move, encode all of the moves before it\n",
    "                    X.append(encode_moves(game.moves[0:i], max_half_turns))\n",
    "                    y.append(move)\n",
    "#                     pdb.set_trace()\n",
    "            counter += 1\n",
    "            if counter % 25 == 0:\n",
    "                print('loaded game {}'.format(counter))\n",
    "            if num_to_load and counter == num_to_load:\n",
    "                break  \n",
    "                \n",
    "#         test_embeddings(X, games, num_to_load) \n",
    "            \n",
    "        num_test = round(len(X) * split_ratio)\n",
    "        X_train, y_train = X[0:num_test], y[0:num_test]\n",
    "        X_test, y_test = X[num_test:], y[num_test:] \n",
    "        \n",
    "        return np.asarray(X_train), np.asarray(y_train), np.asarray(X_test), np.asarray(y_test)\n",
    "    else: \n",
    "        #parse w/ pgnparser\n",
    "        pass\n",
    "    \n",
    "def from_categorical(l):\n",
    "    return np.argmax(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_embeddings(X, games, num_to_test):\n",
    "    print('{} games'.format(len(games)))\n",
    "    move_count = 0\n",
    "    for i, game in enumerate(games):\n",
    "        move_count = move_count + len(game.moves)\n",
    "        if len(game.moves) > 1:\n",
    "            move = game.moves[-2]\n",
    "            if move in word_to_id:\n",
    "                print('move {}'.format(move_count))\n",
    "                print(move)\n",
    "                print(len(X[move_count - 1]), len(game.moves) - 2)\n",
    "                vec = embeddings[word_to_id[move]]\n",
    "                matching_move = X[move_count - 1][len(game.moves) - 2]\n",
    "    #             print(vec)\n",
    "    #             print(matching_move)\n",
    "                assert (vec == matching_move).all\n",
    "            else:\n",
    "                print('{} not in vocab'.format(move))\n",
    "    #         print('tested {}'.format(i))\n",
    "            if i == num_to_test - 2:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 17723 games\n",
      "longest game is 315 half turns\n",
      "loaded game 25\n",
      "loaded game 50\n",
      "loaded game 75\n",
      "loaded game 100\n",
      "loaded game 125\n",
      "loaded game 150\n",
      "loaded game 175\n",
      "loaded game 200\n",
      "loaded game 225\n",
      "loaded game 250\n",
      "loaded game 275\n",
      "loaded game 300\n",
      "loaded game 325\n",
      "loaded game 350\n",
      "loaded game 375\n",
      "loaded game 400\n",
      "loaded game 425\n",
      "loaded game 450\n",
      "loaded game 475\n",
      "loaded game 500\n",
      "loaded game 525\n",
      "loaded game 550\n",
      "loaded game 575\n",
      "loaded game 600\n",
      "loaded game 625\n",
      "loaded game 650\n",
      "loaded game 675\n",
      "loaded game 700\n",
      "loaded game 725\n",
      "loaded game 750\n",
      "loaded game 775\n",
      "loaded game 800\n",
      "loaded game 825\n",
      "loaded game 850\n",
      "loaded game 875\n",
      "loaded game 900\n",
      "loaded game 925\n",
      "loaded game 950\n",
      "loaded game 975\n",
      "loaded game 1000\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train_labels, X_test, y_test_labels = load_data('../data/some.pickle', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.resize(X_train, (X_train.shape[0], 3150))\n",
    "X_test = np.resize(X_test, (X_test.shape[0], 3150))\n",
    "\n",
    "## label one-hot encoding\n",
    "# y_train = to_categorical([word_to_id[w] for w in y_train_labels], len(labels))\n",
    "# y_test = to_categorical([word_to_id[w] for w in y_test_labels], len(labels))\n",
    "\n",
    "## label int encoding\n",
    "y_train = [word_to_id[w] for w in y_train_labels]\n",
    "y_test = [word_to_id[w] for w in y_test_labels]\n",
    "\n",
    "# print(X_train.shape, len(y_train))\n",
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "clf = SVC()\n",
    "clf.fit(X_train[0:1000], y_train[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = [id_to_word[index] for index in [from_categorical(c) for c in predicted]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real = y_test_labels[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nd1 O-O\n",
      "Nc3 O-O\n",
      "Rfd8 O-O\n",
      "Rxd8+ O-O\n",
      "Rg5 O-O\n",
      "Nf6 O-O\n",
      "Ke7 O-O\n",
      "1-0 O-O\n",
      "Ree3 O-O\n",
      "hxg4 O-O\n",
      "Rgf2 O-O\n",
      "Rd4 O-O\n",
      "Re3 O-O\n",
      "d4 O-O\n",
      "Rd1 O-O\n",
      "Rd2 O-O\n",
      "h3 O-O\n",
      "Re2 O-O\n",
      "Kb3 O-O\n",
      "Re1 O-O\n",
      "c5 O-O\n",
      "Bg6 O-O\n",
      "Re8+ O-O\n",
      "Rfd3 O-O\n",
      "a5 O-O\n",
      "Rb1 O-O\n",
      "Kf8 O-O\n",
      "Rf4 O-O\n",
      "Bg7 O-O\n",
      "Be5 O-O\n",
      "Qxd4 O-O\n",
      "Bd4 O-O\n",
      "Be3 O-O\n",
      "Rxd8 O-O\n",
      "Rg1 O-O\n",
      "Rxa2 O-O\n",
      "g6 O-O\n",
      "c6 O-O\n",
      "Na6 O-O\n",
      "Re5 O-O\n",
      "Nf3 O-O\n",
      "h5 O-O\n",
      "f3 O-O\n",
      "Rff3 O-O\n",
      "c4 O-O\n",
      "e5 O-O\n",
      "Rff2 O-O\n",
      "dxe5 O-O\n",
      "cxb4 O-O\n",
      "Na4 O-O\n",
      "d6 O-O\n",
      "Bxe5 O-O\n",
      "e4 O-O\n",
      "Qb6 O-O\n",
      "exd4 O-O\n",
      "Rxd1 O-O\n",
      "Rd3 O-O\n",
      "Qxd1 O-O\n",
      "Ng4 O-O\n",
      "bxc3 O-O\n",
      "Nc5 O-O\n",
      "a4 O-O\n",
      "b5 O-O\n",
      "Be2 O-O\n",
      "Rge3 O-O\n",
      "g5 O-O\n",
      "Be6 O-O\n",
      "a3 O-O\n",
      "Rh1 O-O\n",
      "b4 O-O\n",
      "Bf4 O-O\n",
      "Bf1 O-O\n",
      "Rd8 O-O\n",
      "Rb3 O-O\n",
      "Rc1 O-O\n",
      "Kf2 O-O\n",
      "Nxd4 O-O\n",
      "Rxc3 O-O\n",
      "Rgg2 O-O\n",
      "Rg2 O-O\n",
      "Ke1 O-O\n",
      "O-O O-O\n",
      "Nxc3 O-O\n"
     ]
    }
   ],
   "source": [
    "for r, p in dict(zip(real, predicted)).items():\n",
    "    print(r, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
